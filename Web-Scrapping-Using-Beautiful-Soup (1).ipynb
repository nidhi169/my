{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d127c456",
   "metadata": {},
   "source": [
    "<h3> Web Scrapping </h3>\n",
    "\n",
    "<b> In all the following questions, you have to use BeautifulSoup to scrape different websites and collect data as per \n",
    "the requirement of the question. </b> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33848400",
   "metadata": {},
   "source": [
    "<b> Importing Libraries </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa7036f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully!!!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Libraries loaded successfully!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6af87f",
   "metadata": {},
   "source": [
    "<b> 1. Write a python program to display all the header tags from ‘en.wikipedia.org/wiki/Main_Page’. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad04eeda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heading(url):\n",
    "    p1 = requests.get(url)\n",
    "    #p1.content\n",
    "    \n",
    "    print(p1.status_code)\n",
    "    \n",
    "    s1 = BeautifulSoup(p1.content, 'html.parser')\n",
    "    #print(s1.prettify())\n",
    "    \n",
    "    h1 = s1.find_all('span', class_=\"mw-headline\")\n",
    "    heading = []\n",
    "    for i in h1:\n",
    "        heading.append(i.get_text())\n",
    "        \n",
    "    print(heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90954da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "[\"From today's featured article\", 'Did you know\\xa0...', 'In the news', 'On this day', \"Today's featured picture\", 'Other areas of Wikipedia', \"Wikipedia's sister projects\", 'Wikipedia languages']\n"
     ]
    }
   ],
   "source": [
    "heading(\"https://en.wikipedia.org/wiki/Main_Page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3584ab",
   "metadata": {},
   "source": [
    "<b> 2. Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. Name, IMDB rating, Year of release) and make data frame. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3768f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IMDB_Movie(url):\n",
    "    p2 = requests.get(url)\n",
    "    #p2.content\n",
    "    \n",
    "    print(p2.status_code)\n",
    "    \n",
    "    s2 = BeautifulSoup(p2.content, \"html.parser\")\n",
    "    #print(s2.prettify())\n",
    "    \n",
    "    t1 = s2.find_all('td', class_='titleColumn') \n",
    "    Movie_Title = []\n",
    "    for i in t1:\n",
    "        for j in i.find_all('a'):\n",
    "            Movie_Title.append(j.text)\n",
    "            \n",
    "    y1 = s2.find_all('span', class_='secondaryInfo') \n",
    "    Year_of_Release = []\n",
    "    for i in y1:\n",
    "        Year_of_Release.append(i.text)\n",
    "            \n",
    "    r1 = s2.find_all('td', class_='ratingColumn imdbRating')\n",
    "    Movie_Rating = []\n",
    "    for i in r1:\n",
    "        for j in i.find_all('strong'):\n",
    "            Movie_Rating.append(j.text) \n",
    "            \n",
    "    IMDB_Movies = pd.DataFrame()\n",
    "    IMDB_Movies['Movie_Title'] = Movie_Title[0:100]\n",
    "    IMDB_Movies['Year_of_Release'] = Year_of_Release[0:100]\n",
    "    IMDB_Movies['Movie_Rating'] = Movie_Rating[0:100]\n",
    "    \n",
    "    print(\"Shape of IMDB Movie Ratings is {} rows and {} columns \\n.\".format(*IMDB_Movies.shape))\n",
    "    print(IMDB_Movies.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f091c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of IMDB Movie Ratings is 100 rows and 3 columns \n",
      ".\n",
      "                Movie_Title Year_of_Release Movie_Rating\n",
      "0  The Shawshank Redemption          (1994)          9.2\n",
      "1             The Godfather          (1972)          9.1\n"
     ]
    }
   ],
   "source": [
    "IMDB_Movie(\"https://www.imdb.com/chart/top/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d85e2c",
   "metadata": {},
   "source": [
    "<b> 3. Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. Name, IMDB rating, Year of release) and make data frame. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5549d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IMDB_Ind(url):\n",
    "    p3 = requests.get(url)\n",
    "    #p3.content\n",
    "    \n",
    "    print(p3.status_code)\n",
    "    \n",
    "    s3 = BeautifulSoup(p3.content, 'html.parser')\n",
    "    #print(s3.prettify())\n",
    "    \n",
    "    t1 = s3.find_all(\"td\", class_='titleColumn')\n",
    "    Movie_Title = []\n",
    "    for i in t1:\n",
    "        for j in i.find_all(\"a\"):\n",
    "            Movie_Title.append(j.text)\n",
    "            \n",
    "    y1 = s3.find_all(\"span\", class_=\"secondaryInfo\")\n",
    "    Year = []\n",
    "    for i in y1:\n",
    "        Year.append(i.text)\n",
    "        \n",
    "    r1 = s3.find_all(\"td\", class_='ratingColumn imdbRating')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        for j in i.find_all(\"strong\"):\n",
    "            Rating.append(j.text)\n",
    "    \n",
    "    IMDB_Movies_IND = pd.DataFrame()\n",
    "    IMDB_Movies_IND['Movie_Title'] = Movie_Title[0:100]\n",
    "    IMDB_Movies_IND['Year_of_Release'] = Year[0:100]\n",
    "    IMDB_Movies_IND['Movie_Rating'] = Rating[0:100]\n",
    "    \n",
    "    print(\"Shape of IMDB Movie Ratings is {} rows and {} columns \\n.\".format(*IMDB_Movies_IND.shape))\n",
    "    print(IMDB_Movies_IND.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1f1388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of IMDB Movie Ratings is 100 rows and 3 columns \n",
      ".\n",
      "       Movie_Title Year_of_Release Movie_Rating\n",
      "0  Pather Panchali          (1955)          8.5\n",
      "1          Nayakan          (1987)          8.5\n"
     ]
    }
   ],
   "source": [
    "IMDB_Ind(\"https://www.imdb.com/india/top-rated-indian-movies/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990335d8",
   "metadata": {},
   "source": [
    "<b> 4. Write a python program to scrap book name, author name, genre and book review of any 5 books from ‘www.bookpage.com’. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ca9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookreview(url):\n",
    "    p4 = requests.get(url)\n",
    "    #p4.content\n",
    "    \n",
    "    print(p4.status_code)\n",
    "    \n",
    "    s4 = BeautifulSoup(p4.content, \"html.parser\")\n",
    "    #print(s4.prettify())\n",
    "    \n",
    "    bn = s4.find_all(\"h4\", class_=\"italic\")\n",
    "    Book_Name = []\n",
    "    for i in bn:\n",
    "        for j in i.find_all(\"a\"):\n",
    "            Book_Name.append(i.text.strip(\"\\n\"))\n",
    "            \n",
    "    an = s4.find_all(\"p\", class_='sans bold')\n",
    "    Author = []\n",
    "    for i in an:\n",
    "        Author.append(i.text.strip(\"\\n\"))\n",
    "    \n",
    "    rn = s4.find_all(\"p\", class_='excerpt')\n",
    "    Review = []\n",
    "    for i in rn:\n",
    "        Review.append(i.text.strip(\"\\n\"))\n",
    "        \n",
    "    Book_Reviews = pd.DataFrame()\n",
    "    Book_Reviews['Book_Name'] = Book_Name[0:5]\n",
    "    Book_Reviews['Author_Name'] = Author[0:5]\n",
    "    Book_Reviews['Review'] = Review[0:5]\n",
    "    \n",
    "    print(\"Shape of Book Review is {} rows and {} columns.\".format(*Book_Reviews.shape))\n",
    "    print(Book_Reviews.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa810d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of Book Review is 5 rows and 3 columns.\n",
      "                     Book_Name        Author_Name  \\\n",
      "0   ★ Instructions for Dancing        Nicola Yoon   \n",
      "1                    Super Fly  Jonathan Balcombe   \n",
      "\n",
      "                                              Review  \n",
      "0  When Evie tries to get rid of her romance nove...  \n",
      "1  From wound-healing maggots to flies that helpe...  \n"
     ]
    }
   ],
   "source": [
    "bookreview(\"https://bookpage.com/reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff35296",
   "metadata": {},
   "source": [
    "<b> 5. Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape: </b>\n",
    "\n",
    "> <b>  i. Top 10 ODI teams in men’s cricket along with the records for matches, points and rating. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20b70a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Men(url):\n",
    "    p5 = requests.get(url)\n",
    "    #p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    #print(s5.prettify())\n",
    "    \n",
    "    t1 = s5.find_all('span', class_='u-hide-phablet')\n",
    "    Team_Name = []\n",
    "    for i in t1:\n",
    "        Team_Name.append(i.text)\n",
    "        \n",
    "    m1 = s5.find_all('td', class_='rankings-block__banner--matches')\n",
    "    Matches = []\n",
    "    for i in m1:\n",
    "        Matches.append(i.text)\n",
    "        \n",
    "    p1 = s5.find_all('td', class_='rankings-block__banner--points')\n",
    "    Point = []\n",
    "    for i in p1:\n",
    "        Point.append(i.text) \n",
    "    \n",
    "    r1 = s5.find_all('td', class_='rankings-block__banner--rating u-text-right')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text.strip('\\n'))\n",
    "    Ratings = [] \n",
    "    Ratings.append(Rating[0].strip())\n",
    "    \n",
    "    m1 = s5.find_all('td', class_='table-body__cell u-center-text')\n",
    "    k = []\n",
    "    for i in range(0,len(m1),2):\n",
    "        k.append(m1[i])\n",
    "    for i in k:\n",
    "        Matches.append(i.get_text())\n",
    "        \n",
    "    k = []\n",
    "    for i in range(1,len(m1),2):\n",
    "        k.append(m1[i])\n",
    "    for i in k:\n",
    "        Point.append(i.get_text())\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell u-text-right rating')\n",
    "    for i in r1:\n",
    "        Ratings.append(i.get_text().strip('\\n'))\n",
    "        \n",
    "    ODI_Men = pd.DataFrame()\n",
    "    ODI_Men['Team_Name'] = Team_Name[0:10]\n",
    "    ODI_Men['Matches'] = Matches[0:10]\n",
    "    ODI_Men['Ratings'] = Ratings[0:10]\n",
    "    ODI_Men['Point'] = Point[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Men.shape))\n",
    "    print(ODI_Men.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d5087dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of dataset is 10 rows and 4 columns\n",
      "     Team_Name Matches Ratings  Point\n",
      "0  New Zealand      17     121  2,054\n",
      "1    Australia      25     118  2,945\n",
      "2        India      29     115  3,344\n"
     ]
    }
   ],
   "source": [
    "ODI_Men(\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51427dc1",
   "metadata": {},
   "source": [
    "> <b>  ii. Top 10 ODI Batsmen in men along with the records of their team and rating. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a613e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Batsman(url):\n",
    "    p5 = requests.get(url)\n",
    "    #p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    #print(s5.prettify())\n",
    "    \n",
    "    p1 = s5.find_all('div', class_='rankings-block__banner--name-large')\n",
    "    Player = []\n",
    "    for i in p1:\n",
    "        Player.append(i.text)\n",
    "    \n",
    "    p2 = s5.find_all('td', class_='table-body__cell rankings-table__name name')\n",
    "    for i in p2:\n",
    "        for j in i.find_all(\"a\"):\n",
    "            Player.append(j.text)\n",
    "            \n",
    "    t1 = s5.find(\"div\", class_=\"rankings-block__banner--nationality\")\n",
    "    Team = []\n",
    "    Team.append(t1.text.strip())\n",
    "    \n",
    "    t2 = s5.find_all(\"span\", class_='table-body__logo-text')\n",
    "    for i in t2:\n",
    "        Team.append(i.text)\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell rating')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    ODI_Batsman = pd.DataFrame()\n",
    "    ODI_Batsman['Team'] = Team[0:10]\n",
    "    ODI_Batsman['Player'] = Player[0:10]\n",
    "    ODI_Batsman['Rating'] = Rating[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Batsman.shape))\n",
    "    print(ODI_Batsman.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ad92a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of dataset is 10 rows and 3 columns\n",
      "  Team        Player Rating\n",
      "0  PAK    Babar Azam    857\n",
      "1  IND   Virat Kohli    825\n",
      "2  IND  Rohit Sharma    801\n"
     ]
    }
   ],
   "source": [
    "ODI_Batsman(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/Batting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b565c",
   "metadata": {},
   "source": [
    "> <b>  iii. Top 10 ODI bowlers along with the records of their team and rating. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bafd8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Bowler(url):\n",
    "    p5 = requests.get(url)\n",
    "    #p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    #print(s5.prettify())\n",
    "    \n",
    "    p1 = s5.find_all('div', class_='rankings-block__banner--name-large')\n",
    "    Player = []\n",
    "    for i in p1:\n",
    "        Player.append(i.text)\n",
    "    \n",
    "    p2 = s5.find_all('td', class_='table-body__cell rankings-table__name name')\n",
    "    for i in p2:\n",
    "        for j in i.find_all(\"a\"):\n",
    "            Player.append(j.text)\n",
    "            \n",
    "    t1 = s5.find(\"div\", class_=\"rankings-block__banner--nationality\")\n",
    "    Team = []\n",
    "    Team.append(t1.text.strip())\n",
    "    \n",
    "    t2 = s5.find_all(\"span\", class_='table-body__logo-text')\n",
    "    for i in t2:\n",
    "        Team.append(i.text)\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell rating')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    ODI_Bowler = pd.DataFrame()\n",
    "    ODI_Bowler['Team'] = Team[0:10]\n",
    "    ODI_Bowler['Player'] = Player[0:10]\n",
    "    ODI_Bowler['Rating'] = Rating[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Bowler.shape))\n",
    "    print(ODI_Bowler.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a869d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of dataset is 10 rows and 3 columns\n",
      "  Team            Player Rating\n",
      "0   NZ       Trent Boult    708\n",
      "1  AFG  Mujeeb Ur Rahman    691\n",
      "2   NZ        Matt Henry    690\n"
     ]
    }
   ],
   "source": [
    "ODI_Batsman(\"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/Bowling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe9f0a9",
   "metadata": {},
   "source": [
    "<b> 6. Write a python program to scrape cricket rankings from ‘www.icc-cricket.com’. You have to scrape: </b>\n",
    "\n",
    "> <b> i. Top 10 ODI teams in women’s cricket along with the records for matches, points and rating. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b2aec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Women(url):\n",
    "    p5 = requests.get(url)\n",
    "    #p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    #print(s5.prettify())\n",
    "    \n",
    "    t1 = s5.find_all('span', class_='u-hide-phablet')\n",
    "    Team_Name = []\n",
    "    for i in t1:\n",
    "        Team_Name.append(i.text)\n",
    "        \n",
    "    m1 = s5.find_all('td', class_='rankings-block__banner--matches')\n",
    "    Matches = []\n",
    "    for i in m1:\n",
    "        Matches.append(i.text)\n",
    "        \n",
    "    p1 = s5.find_all('td', class_='rankings-block__banner--points')\n",
    "    Point = []\n",
    "    for i in p1:\n",
    "        Point.append(i.text) \n",
    "    \n",
    "    r1 = s5.find_all('td', class_='rankings-block__banner--rating u-text-right')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text.strip('\\n'))\n",
    "    Ratings = [] \n",
    "    Ratings.append(Rating[0].strip())\n",
    "    \n",
    "    m1 = s5.find_all('td', class_='table-body__cell u-center-text')\n",
    "    k = []\n",
    "    for i in range(0,len(m1),2):\n",
    "        k.append(m1[i])\n",
    "    for i in k:\n",
    "        Matches.append(i.get_text())\n",
    "        \n",
    "    k = []\n",
    "    for i in range(1,len(m1),2):\n",
    "        k.append(m1[i])\n",
    "    for i in k:\n",
    "        Point.append(i.get_text())\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell u-text-right rating')\n",
    "    for i in r1:\n",
    "        Ratings.append(i.get_text().strip('\\n'))\n",
    "        \n",
    "    ODI_Women = pd.DataFrame()\n",
    "    ODI_Women['Team_Name'] = Team_Name[0:10]\n",
    "    ODI_Women['Matches'] = Matches[0:10]\n",
    "    ODI_Women['Ratings'] = Ratings[0:10]\n",
    "    ODI_Women['Point'] = Point[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Women.shape))\n",
    "    print(ODI_Women.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9ddd945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of dataset is 10 rows and 4 columns\n",
      "      Team_Name Matches Ratings  Point\n",
      "0     Australia      18     164  2,955\n",
      "1  South Africa      24     118  2,828\n",
      "2       England      17     117  1,993\n"
     ]
    }
   ],
   "source": [
    "ODI_Women(\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ded948",
   "metadata": {},
   "source": [
    "> <b> ii. Top 10 women’s ODI players along with the records of their team and rating. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bda21d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Woman(url):\n",
    "    p5 = requests.get(url)\n",
    "    #p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    #print(s5.prettify())\n",
    "    \n",
    "    p1 = s5.find_all('div', class_='rankings-block__banner--name-large')\n",
    "    Player = []\n",
    "    for i in p1:\n",
    "        Player.append(i.text)\n",
    "    \n",
    "    p2 = s5.find_all('td', class_='table-body__cell rankings-table__name name')\n",
    "    for i in p2:\n",
    "        for j in i.find_all(\"a\"):\n",
    "            Player.append(j.text)\n",
    "            \n",
    "    t1 = s5.find(\"div\", class_=\"rankings-block__banner--nationality\")\n",
    "    Team = []\n",
    "    Team.append(t1.text.strip())\n",
    "    \n",
    "    t2 = s5.find_all(\"span\", class_='table-body__logo-text')\n",
    "    for i in t2:\n",
    "        Team.append(i.text)\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell rating')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    ODI_Woman = pd.DataFrame()\n",
    "    ODI_Woman['Team'] = Team[0:10]\n",
    "    ODI_Woman['Player'] = Player[0:10]\n",
    "    ODI_Woman['Rating'] = Rating[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Woman.shape))\n",
    "    print(ODI_Woman.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b066fff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of dataset is 10 rows and 3 columns\n",
      "  Team          Player Rating\n",
      "0  ENG  Tammy Beaumont    758\n",
      "1   SA     Lizelle Lee    756\n",
      "2  AUS    Alyssa Healy    746\n"
     ]
    }
   ],
   "source": [
    "ODI_Woman(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe883f83",
   "metadata": {},
   "source": [
    "> <b> iii. Top 10 women’s ODI all-rounder along with the records of their team and rating. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d74abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODI_Allrounder(url):\n",
    "    p5 = requests.get(url)\n",
    "    #p5.content\n",
    "    print(p5.status_code)\n",
    "    \n",
    "    s5 = BeautifulSoup(p5.content, \"html.parser\")\n",
    "    #print(s5.prettify())\n",
    "    \n",
    "    p1 = s5.find_all('div', class_='rankings-block__banner--name-large')\n",
    "    Player = []\n",
    "    for i in p1:\n",
    "        Player.append(i.text)\n",
    "    \n",
    "    p2 = s5.find_all('td', class_='table-body__cell rankings-table__name name')\n",
    "    for i in p2:\n",
    "        for j in i.find_all(\"a\"):\n",
    "            Player.append(j.text)\n",
    "            \n",
    "    t1 = s5.find(\"div\", class_=\"rankings-block__banner--nationality\")\n",
    "    Team = []\n",
    "    Team.append(t1.text.strip())\n",
    "    \n",
    "    t2 = s5.find_all(\"span\", class_='table-body__logo-text')\n",
    "    for i in t2:\n",
    "        Team.append(i.text)\n",
    "        \n",
    "    r1 = s5.find_all('td', class_='table-body__cell rating')\n",
    "    Rating = []\n",
    "    for i in r1:\n",
    "        Rating.append(i.text)\n",
    "        \n",
    "    ODI_Allrounder = pd.DataFrame()\n",
    "    ODI_Allrounder['Team'] = Team[0:10]\n",
    "    ODI_Allrounder['Player'] = Player[0:10]\n",
    "    ODI_Allrounder['Rating'] = Rating[0:10]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*ODI_Allrounder.shape))\n",
    "    print(ODI_Allrounder.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4dcc6109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of dataset is 10 rows and 3 columns\n",
      "  Team           Player Rating\n",
      "0   SA   Marizanne Kapp    418\n",
      "1  AUS     Ellyse Perry    410\n",
      "2   WI  Stafanie Taylor    349\n"
     ]
    }
   ],
   "source": [
    "ODI_Allrounder(\"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca2bd6",
   "metadata": {},
   "source": [
    "<b> 7. Write a python program to scrape details of all the mobile phones under Rs. 20,000 listed on Amazon.in. The scraped data should include Product Name, Price, Image URL and Average Rating. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29d9a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = requests.get(\"https://www.amazon.in/s?k=mobile&rh=p_36%3A-20000000&qid=1621975129&rnid=1318502031&ref=sr_nr_p_36_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "706f949e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5f5d04ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Amazon(url):\n",
    "    p7 = requests.get(url)\n",
    "    #p5.content\n",
    "    print(p7.status_code)\n",
    "    \n",
    "    s7 = BeautifulSoup(p7.content, 'html.parser')\n",
    "    #print(s7.prettify())\n",
    "    \n",
    "    m_name = s7.find_all('span', class_='a-size-medium a-color-base a-text-normal')\n",
    "    m_price = s7.find_all('span', class_='a-price-whole')\n",
    "    m_rating = s7.find_all('span', class_=\"a-icon-alt\")\n",
    "    m_img = s7.find_all('img', class_=\"s-image\")\n",
    "    \n",
    "    Mobile = []\n",
    "    for i in m_name:\n",
    "        Mobile.append(i.get_text())\n",
    "        \n",
    "    Price = []\n",
    "    for i in m_price:\n",
    "        Price.append(i.get_text())\n",
    "        \n",
    "    Rating = []\n",
    "    for i in m_rating:\n",
    "        Rating.append(i.get_text())\n",
    "        \n",
    "    Image = []\n",
    "    for i in m_img:\n",
    "        Image.append(i.get('srcset'))\n",
    "        \n",
    "    Mobile_dataset = pd.DataFrame()\n",
    "    Mobile_dataset['Mobile_Name'] = Mobile[0:16]\n",
    "    Mobile_dataset['Mobile_Price'] = Price[0:16]\n",
    "    Mobile_dataset['Mobile_rating'] = Rating[0:16]\n",
    "    Mobile_dataset['Image_Link'] = Image[0:16]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*Mobile_dataset.shape))\n",
    "    print(Mobile_dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d5bfdb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of dataset is 16 rows and 4 columns\n",
      "                                         Mobile_Name Mobile_Price  \\\n",
      "0  Redmi 9A (Nature Green, 2GB RAM, 32GB Storage)...        6,999   \n",
      "1  Redmi 9 (Sky Blue, 4GB RAM, 64GB Storage)| 500...        8,799   \n",
      "2  Samsung Galaxy M12 (Blue,4GB RAM, 64GB Storage...       10,999   \n",
      "3  Samsung Galaxy M31 (Ocean Blue, 6GB RAM, 128GB...       14,999   \n",
      "4  Oppo A31 (Mystery Black, 6GB RAM, 128GB Storag...       10,990   \n",
      "\n",
      "        Mobile_rating                                         Image_Link  \n",
      "0  4.2 out of 5 stars                                               None  \n",
      "1  4.2 out of 5 stars  https://m.media-amazon.com/images/I/71sxlhYhKW...  \n",
      "2  4.0 out of 5 stars  https://m.media-amazon.com/images/I/71A9Vo1Bat...  \n",
      "3  4.3 out of 5 stars  https://m.media-amazon.com/images/I/71yYaNztZ0...  \n",
      "4  4.2 out of 5 stars  https://m.media-amazon.com/images/I/71-Su4Wr0H...  \n"
     ]
    }
   ],
   "source": [
    "Amazon(\"https://www.amazon.in/s?k=mobile&rh=p_36%3A-20000000&qid=1621975129&rnid=1318502031&ref=sr_nr_p_36_5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4c6e47",
   "metadata": {},
   "source": [
    "<b> 8. Write a python program to extract information about the local weather from the National Weather Service website of USA, https://www.weather.gov/ for the city, San Francisco. You need to extract data about 7 day extended forecast display for the city. The data should include period, short description, temperature and description. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02898f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Weather(url):\n",
    "    p8 = requests.get(url)\n",
    "    #p8.content\n",
    "    print(p8.status_code)\n",
    "    \n",
    "    s8 = BeautifulSoup(p8.content, \"html.parser\")\n",
    "    #print(s8.prettify())\n",
    "    \n",
    "    d1 = s8.find_all(\"p\", class_ =\"period-name\")\n",
    "    Day = []\n",
    "    for i in d1:\n",
    "        Day.append(i.text)\n",
    "\n",
    "    t1 = s8.find_all('p', class_='temp temp-high')\n",
    "    Temp = []\n",
    "    for i in t1:\n",
    "        Temp.append(i.text)\n",
    "        \n",
    "    d2 = s8.find_all(\"p\", class_=\"short-desc\")\n",
    "    Description = []\n",
    "    for i in d2:\n",
    "        Description.append(i.text)\n",
    "        \n",
    "    Weather = pd.DataFrame()\n",
    "    Weather['Day'] = Day[0:5]\n",
    "    Weather['Temp'] = Temp[0:5]\n",
    "    Weather['Description'] = Description[0:5]\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*Weather.shape))\n",
    "    print(Weather.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ba67e5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of dataset is 5 rows and 3 columns\n",
      "              Day         Temp                Description\n",
      "0   ThisAfternoon  High: 64 °F            Sunny andBreezy\n",
      "1         Tonight  High: 64 °F               Mostly Clear\n",
      "2       Wednesday  High: 66 °F  Sunny thenSunny andBreezy\n",
      "3  WednesdayNight  High: 66 °F     Mostly Clearand Breezy\n",
      "4        Thursday  High: 64 °F            Sunny andBreezy\n"
     ]
    }
   ],
   "source": [
    "Weather(\"https://forecast.weather.gov/MapClick.php?lat=37.777120000000025&lon=-122.41963999999996#.YK1i9agzZPZ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dc870d",
   "metadata": {},
   "source": [
    "<b> 9. Write a python program to scrape fresher job listings from ‘https://internshala.com/’. It should include job title, \n",
    "company name, CTC, and apply date. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b51e8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Internshala(url):\n",
    "    p9 = requests.get(url)\n",
    "    #p9.content\n",
    "    print(p9.status_code)\n",
    "    \n",
    "    s9 = BeautifulSoup(p9.content, \"html.parser\")\n",
    "    #print(s9.prettify())\n",
    "    \n",
    "    title = []\n",
    "    company = []\n",
    "    ctc = []\n",
    "    apply_date = []\n",
    "    \n",
    "    temp = []\n",
    "    \n",
    "    get_titles = s9.find_all('div',class_ = \"heading_4_5 profile\")   \n",
    "    get_companies = s9.find_all('a', class_ = 'link_display_like_text')\n",
    "    get_ctcadate = s9.find_all('div',class_ = \"item_body\", id = False)\n",
    "    \n",
    "    for cdate in get_ctcadate:\n",
    "        temp.append(cdate.text.strip())\n",
    "        \n",
    "    for t, c in zip(get_titles, get_companies):\n",
    "        title.append(t.text.strip())\n",
    "        company.append(c.text.strip())\n",
    "    \n",
    "    for i in temp:\n",
    "        ctc.append(i) if 'LPA' in i else apply_date.append(i)\n",
    "            \n",
    "    Internshala = pd.DataFrame({'Title': title,\n",
    "                     'Company': company,\n",
    "                     'CTC': ctc,\n",
    "                     'Apply Date': apply_date\n",
    "                     })\n",
    "    print(\"Shape of dataset is {} rows and {} columns\".format(*Internshala.shape))\n",
    "    print(Internshala.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70bfe277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "Shape of dataset is 40 rows and 4 columns\n",
      "                          Title          Company        CTC  Apply Date\n",
      "0  Talent Acquisition Executive        CrewKarma  3 - 5 LPA  24 Jun' 21\n",
      "1          Full Stack Developer          Codfirm  5 - 8 LPA  24 Jun' 21\n",
      "2             Backend Developer          WebMOBI      3 LPA  24 Jun' 21\n",
      "3          Full Stack Developer  TMBill Software      3 LPA  24 Jun' 21\n",
      "4        Junior Sales Associate       Kraftshala      4 LPA  24 Jun' 21\n"
     ]
    }
   ],
   "source": [
    "Internshala(\"https://internshala.com/fresher-jobs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5d3b5",
   "metadata": {},
   "source": [
    "<b> The End!!! </b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
